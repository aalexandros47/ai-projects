{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7858fdb6-3939-4cc1-882d-c0815638446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (1.70.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (2.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from openai) (4.13.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnob\\documents\\ai-research-assistant\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai beautifulsoup4 requests python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d924d4d-0c51-43d2-b2db-4b29e2470364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Connect to OpenAI\n",
    "openai = OpenAI(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121c99ae-d012-4f38-acda-5da4186792c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(query, max_results=3):\n",
    "    print(f\"üîç Searching DuckDuckGo for: {query}\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(f\"https://html.duckduckgo.com/html/?q={query}\", headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    results = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", class_=\"result__a\", href=True):\n",
    "        url = link[\"href\"]\n",
    "        if url.startswith(\"http\"):\n",
    "            results.append(url)\n",
    "        if len(results) >= max_results:\n",
    "            break\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44a94f7-df07-4b00-8abc-8459e766082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "    try:\n",
    "        print(f\"üßΩ Scraping URL: {url}\")\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join([para.get_text() for para in paragraphs])\n",
    "        return text[:5000]  # limit to 5000 characters\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error scraping:\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a59c5d8-8b16-425a-876e-5a146a52c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    try:\n",
    "        print(\"üß† Summarizing content...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"GPT-4o mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful academic assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n{text}\"},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error summarizing:\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb67d840-a974-4e97-b91e-d9ae0f8536b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_report(topic, summaries):\n",
    "    report = f\"# Research Summary: {topic}\\n\\n\"\n",
    "    for i, item in enumerate(summaries, 1):\n",
    "        report += f\"## Source {i}: {item['url']}\\n{item['summary']}\\n\\n\"\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92271dd6-683c-4c96-bdd3-bef38409d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown_report(report, filename=\"output.md\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"‚úÖ Markdown report saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a70da9a4-1a86-4da6-a4f4-f16aedec72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def convert_to_pdf(input_file=\"output.md\", output_file=\"output.pdf\"):\n",
    "    try:\n",
    "        subprocess.run([\"pandoc\", input_file, \"-o\", output_file], check=True)\n",
    "        print(f\"‚úÖ PDF saved as {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå PDF conversion failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e349c4-d3cf-4d8c-a525-0cfe76a04504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching DuckDuckGo for: Future of Artificial Intelligence\n",
      "‚úÖ Markdown report saved as output.md\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step execution\n",
    "topic = \"Future of Artificial Intelligence\"\n",
    "urls = search_web(topic)\n",
    "\n",
    "summaries = []\n",
    "for url in urls:\n",
    "    text = scrape_url(url)\n",
    "    if text:\n",
    "        summary = summarize_text(text)\n",
    "        summaries.append({\"url\": url, \"summary\": summary})\n",
    "\n",
    "report = organize_report(topic, summaries)\n",
    "save_markdown_report(report)\n",
    "\n",
    "# Optional: Convert to PDF\n",
    "# convert_to_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f2a92-b72e-4722-bfcd-326824f8fb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Assistant)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
